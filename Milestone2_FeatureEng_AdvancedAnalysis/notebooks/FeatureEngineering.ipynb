{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from itertools import combinations\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned = pd.read_csv('https://raw.githubusercontent.com/MohamedMostafa259/Customer-Churn-Prediction-and-Analysis/main/Milestone1_DataCollection_EDA_DataCleaning/data/train_cleaned_imputed.csv')\n",
    "train_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a copy for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_copy = train_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_copy.select_dtypes('number').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_copy['points_per_transaction'] = train_cleaned_copy['points_in_wallet'] / train_cleaned_copy['avg_transaction_value']\n",
    "train_cleaned_copy['transaction_value_per_time_unit'] = train_cleaned_copy['avg_transaction_value'] / train_cleaned_copy['avg_time_spent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_of_day(hour):\n",
    "\tif 5 <= hour < 12:\n",
    "\t\treturn 'Morning'\n",
    "\telif 12 <= hour < 17:\n",
    "\t\treturn 'Afternoon'\n",
    "\telif 17 <= hour < 21:\n",
    "\t\treturn 'Evening'\n",
    "\telse:\n",
    "\t\treturn 'Night'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ampm_mapping(hour):\n",
    "\tif 0 <= hour < 12:\n",
    "\t\treturn 'AM'\n",
    "\telse:\n",
    "\t\treturn 'PM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_copy['last_visit_hour'] = pd.to_datetime(train_cleaned_copy['last_visit_time']).dt.hour\n",
    "train_cleaned_copy['last_visit_time_of_day'] = train_cleaned_copy['last_visit_hour'].apply(time_of_day)\n",
    "train_cleaned_copy['last_visit_AMPM'] = train_cleaned_copy['last_visit_hour'].apply(ampm_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_copy['joining_date'] = pd.to_datetime(train_cleaned_copy['joining_date'])\n",
    "train_cleaned_copy['joining_day_name'] = train_cleaned_copy['joining_date'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_copy['is_weekend'] = ((train_cleaned_copy['joining_day_name'] == 'Sunday') | \n",
    "\t\t\t\t\t\t\t\t\t(train_cleaned_copy['joining_day_name'] == 'Saturday')).astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data (ready for advanced analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_copy.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_copy.to_csv('train_basicFeatureEng.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_copy.select_dtypes(np.number).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_cols = ['age', 'days_since_last_login', 'avg_time_spent', 'avg_transaction_value', \n",
    "\t\t\t  'avg_frequency_login_days', 'points_in_wallet', 'points_per_transaction', \n",
    "              'transaction_value_per_time_unit', 'last_visit_hour']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_cleaned_copy.select_dtypes(exclude='number').columns:\n",
    "\tif col in ['joining_date', 'last_visit_time']:\n",
    "\t\tcontinue\n",
    "\tprint(f'{col} Column', '-'*50)\n",
    "\tprint(train_cleaned_copy[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_cols = list(set(train_cleaned_copy.select_dtypes(exclude='number').columns.tolist()) - \n",
    "\t\t\t\t\t{'joining_date', 'last_visit_time'} - {'membership_category', 'feedback'})\n",
    "one_hot_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding (includes binary encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_copy = pd.get_dummies(train_cleaned_copy, columns=one_hot_cols, drop_first=True, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinal_cols = ['membership_category', 'feedback']\n",
    "train_cleaned_copy['membership_category'] = pd.Categorical(train_cleaned_copy['membership_category'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   categories=['No Membership', 'Basic Membership', 'Silver Membership', \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   'Gold Membership', 'Platinum Membership', 'Premium Membership'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tordered=True).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_feedback = ['Products always in Stock', 'Quality Customer Care', 'Reasonable Price', 'User Friendly Website']\n",
    "negative_feedback = ['Poor Website' ,'Poor Customer Service', 'Poor Product Quality', 'Too many ads']\n",
    "neutral_feedback = ['No reason specified']\n",
    "\n",
    "def get_sentiment(feedback):\n",
    "\tif feedback in positive_feedback:\n",
    "\t\treturn 1\n",
    "\telif feedback in negative_feedback:\n",
    "\t\treturn -1\n",
    "\telse:\n",
    "\t\treturn 0 # neutral\n",
    "\t\n",
    "train_cleaned_copy['feedback'] = train_cleaned_copy['feedback'].transform(get_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General transformations numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_col in train_cleaned_copy.select_dtypes(include=np.number).columns:\n",
    "\ttrain_cleaned_copy[f'{num_col}_sqrt'] = np.sqrt(train_cleaned_copy[num_col] + 1)\n",
    "\ttrain_cleaned_copy[f'{num_col}_square'] = np.square(train_cleaned_copy[num_col])\n",
    "\ttrain_cleaned_copy[f'{num_col}_log'] = np.log1p(train_cleaned_copy[num_col] + 0.01)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is from the EDA.ipynb notebook in milestone 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# log1p(x) = log(x + 1): this avoids errors when x = 0\n",
    "train_cleaned_copy['log_avg_time_spent'] = np.log1p(train_cleaned_copy['avg_time_spent'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "sns.violinplot(train_cleaned_copy['avg_time_spent'], ax=axes[0])\n",
    "sns.violinplot(train_cleaned_copy['log_avg_time_spent'], ax=axes[1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "train_cleaned_copy.corr(numeric_only=True)['churn_risk_score'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_copy.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features that we may need to transform are:\n",
    "\n",
    "- `avg_time_spent`:\n",
    "\n",
    "\tavg_time_spent                                       -0.027045\n",
    "\n",
    "\tavg_time_spent_sqrt                                  -0.031828\n",
    "\n",
    "\tavg_time_spent_log                                   -0.032828\n",
    "\n",
    "- `avg_transaction_value`:\n",
    "\n",
    "\tavg_transaction_value                                -0.362539\n",
    "\n",
    "\tavg_transaction_value_square                         -0.429874"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_cols.append('avg_transaction_value_square')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_cleaned_imputed = pd.read_csv('https://raw.githubusercontent.com/MohamedMostafa259/Customer-Churn-Prediction-and-Analysis/main/Milestone1_DataCollection_EDA_DataCleaning/data/train_split_cleaned_imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_cleaned_imputed.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_split_cleaned_imputed.drop(columns=['churn_risk_score'])\n",
    "y_train = train_split_cleaned_imputed['churn_risk_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureEng custom transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is DataFrame of training features\n",
    "# r_list is a list of desired combination sizes\n",
    "# returns DataFrame with new combination features, excluding original categorical columns\n",
    "def create_categorical_combinations(X, r_list, cat_cols):\n",
    "\tdf_str = X[cat_cols].astype(str) # to allow concatenation\n",
    "\tfor r in r_list:\n",
    "\t\t# generate all combinations of length r\n",
    "\t\tcombinations_iter = combinations(cat_cols, r)\n",
    "\t\tfor comb in combinations_iter:\n",
    "\t\t\tdf_str['+'.join(comb)] = df_str[list(comb)].agg(''.join, axis=1)\n",
    "\treturn df_str.drop(columns=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEng(BaseEstimator, TransformerMixin):\n",
    "\tdef __init__(self):\n",
    "\t\tself.membership_order = ['No Membership', 'Basic Membership', 'Silver Membership',\n",
    "\t\t\t\t\t\t\t\t 'Gold Membership', 'Platinum Membership', 'Premium Membership']\n",
    "\t\tself.positive_feedback = ['Products always in Stock', 'Quality Customer Care', 'Reasonable Price', 'User Friendly Website']\n",
    "\t\tself.negative_feedback = ['Poor Website', 'Poor Customer Service', 'Poor Product Quality', 'Too many ads']\n",
    "\n",
    "\tdef time_of_day(self, hour):\n",
    "\t\tif 5 <= hour < 12:\n",
    "\t\t\treturn 'Morning'\n",
    "\t\telif 12 <= hour < 17:\n",
    "\t\t\treturn 'Afternoon'\n",
    "\t\telif 17 <= hour < 21:\n",
    "\t\t\treturn 'Evening'\n",
    "\t\telse:\n",
    "\t\t\treturn 'Night'\n",
    "\t\n",
    "\tdef ampm_mapping(self, hour):\n",
    "\t\treturn 'AM' if 0 <= hour < 12 else 'PM'\n",
    "\n",
    "\tdef get_sentiment(self, feedback):\n",
    "\t\tif feedback in self.positive_feedback:\n",
    "\t\t\treturn 1\n",
    "\t\telif feedback in self.negative_feedback:\n",
    "\t\t\treturn -1\n",
    "\t\telse:\n",
    "\t\t\treturn 0\n",
    "\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, X):\n",
    "\t\tX = X.copy()\n",
    "\t\t\n",
    "\t\tX['points_per_transaction'] = X['points_in_wallet'] / X['avg_transaction_value']\n",
    "\t\tX['transaction_value_per_time_unit'] = X['avg_transaction_value'] / X['avg_time_spent']\n",
    "\t\t\n",
    "\t\tX['last_visit_hour'] = pd.to_datetime(X['last_visit_time']).dt.hour\n",
    "\t\tX['last_visit_time_of_day'] = X['last_visit_hour'].apply(self.time_of_day)\n",
    "\t\tX['last_visit_AMPM'] = X['last_visit_hour'].apply(self.ampm_mapping)\n",
    "\t\tX.drop('last_visit_time', axis=1, inplace=True)\n",
    "\t\t\n",
    "\t\tX['joining_date'] = pd.to_datetime(X['joining_date'])\n",
    "\t\tX['joining_day_name'] = X['joining_date'].dt.day_name()\n",
    "\t\tX['is_weekend'] = X['joining_day_name'].isin(['Saturday', 'Sunday']).astype(int)\n",
    "\t\tX.drop('joining_date', axis=1, inplace=True)\n",
    "\t\t\n",
    "\t\tcat_cols = list(X.select_dtypes(include=['object', 'category']).columns)\n",
    "\t\tcat_combos_df = create_categorical_combinations(X, range(2, 3), cat_cols)\n",
    "\t\tX = pd.concat([X, cat_combos_df], axis=1)\n",
    "\t\t\n",
    "\t\tX['membership_category'] = pd.Categorical( X['membership_category'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  categories=self.membership_order, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  ordered=True).codes\n",
    "\t\t\n",
    "\t\tX['feedback'] = X['feedback'].apply(self.get_sentiment)\n",
    "\t\n",
    "\t\t \n",
    "\t\tX['avg_time_spent_log'] = np.log1p(X['avg_time_spent'])  \n",
    "\t\tX['avg_transaction_value_square'] = np.square(X['avg_transaction_value'])\n",
    "\t\t\n",
    "\t\treturn X\n",
    "\t\n",
    "\tdef fit_transform(self, X, y=None):\n",
    "\t\tX_transformed = self.transform(X)\n",
    "\t\tself.feature_names_out_ = X_transformed.columns\n",
    "\t\treturn X_transformed\n",
    "\t\n",
    "\tdef get_feature_names_out(self, input_features=None):\n",
    "\t\treturn self.feature_names_out_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder_scaler transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_scaler_transformer = ColumnTransformer([\n",
    "\t('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=int), \n",
    "  make_column_selector(dtype_include=['object'])), \n",
    "\t('scaler', StandardScaler(), scale_cols)\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge transformers into a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureEng_encoder_scaler_pipeline = Pipeline([\n",
    "    ('featureEng', FeatureEng()), \n",
    "    ('encoder_scaler', encoder_scaler_transformer)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = featureEng_encoder_scaler_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(featureEng_encoder_scaler_pipeline.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isinf(X_train_preprocessed).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = pd.DataFrame(X_train_preprocessed, \n",
    "                                    columns=featureEng_encoder_scaler_pipeline.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection & Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopImportanceFeatures(BaseEstimator, TransformerMixin):\n",
    "\tdef __init__(self, model, threshold=0.95):\n",
    "\t\tself.model = model\n",
    "\t\tself.threshold = threshold\n",
    "\t\t\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\tself.model.fit(X, y)\n",
    "\t\timportances = self.model.feature_importances_\n",
    "\t\tsorted_idx = np.argsort(importances)[::-1]\n",
    "\t\tcumulative_importances = importances[sorted_idx].cumsum()\n",
    "\t\tcutoff_idx = np.searchsorted(cumulative_importances, self.threshold) + 1\n",
    "\t\tself.top_features_ = sorted_idx[:cutoff_idx]\n",
    "\t\treturn self\n",
    "\t\n",
    "\tdef transform(self, X):\n",
    "\t\treturn X.iloc[:, self.top_features_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipe = Pipeline([\n",
    "    ('selector', TopImportanceFeatures(XGBClassifier(random_state=42), 0.3)), \n",
    "    ('model', XGBClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'selector__threshold': np.arange(0.05, 1, 0.05)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model_pipe, param_grid=param_grid, verbose=5, cv=3, return_train_score=True)\n",
    "\n",
    "# target labels (y_train) starts from '1': [1, 2, 3, 4, 5], \n",
    "# but XGBoost expects them to start from 0, like [0, 1, 2, 3, 4].\n",
    "grid_search.fit(X_train_preprocessed, y_train-1)\n",
    "# don't forget to add one in the prediction time:\n",
    "# y_pred = xgb_clf.predict(X_test_preprocessed) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grid_search.best_estimator_[1].feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selector = grid_search.best_estimator_[0].model\n",
    "len(feature_selector.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = feature_selector.feature_importances_\n",
    "feature_names = X_train_preprocessed.columns\n",
    "feature_importance_df = pd.DataFrame({'feature':feature_names, 'importance':importances})\n",
    "feature_importance_df['importance'] = feature_importance_df['importance'].round(5)\n",
    "feature_importance_df = feature_importance_df.sort_values('importance', ascending=False).reset_index()\n",
    "feature_importance_df['cumulative_importance'] = feature_importance_df['importance'].cumsum()\n",
    "feature_importance_df[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model uses only two original features: `membership_category` and `feedback`.\n",
    "\n",
    "Let's try to perform grid search, this time on a specific range → (0.15, 0.21, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipe2 = Pipeline([\n",
    "    ('selector', TopImportanceFeatures(XGBClassifier(random_state=42), 0.3)), \n",
    "    ('model', XGBClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid2 = {\n",
    "    'selector__threshold': np.arange(0.15, 0.21, 0.01)\n",
    "}\n",
    "\n",
    "grid_search2 = GridSearchCV(model_pipe, param_grid=param_grid2, verbose=5, cv=4, return_train_score=True)\n",
    "\n",
    "# target labels (y_train_split) starts from '1': [1, 2, 3, 4, 5], \n",
    "# but XGBoost expects them to start from 0, like [0, 1, 2, 3, 4].\n",
    "grid_search2.fit(X_train_preprocessed, y_train-1)\n",
    "# don't forget to add one in the prediction time:\n",
    "# y_pred = xgb_clf.predict(X_test_preprocessed) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grid_search2.best_estimator_[1].feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase, the number of features, the gap between train and validation scores increases with a very little increase in the validation score. So, There is a risk of overfitting. That's why I will just stick with the best two features: `membership_category` and `feedback`\n",
    "\n",
    "Let's rebuild our `FeatureEng` class to remove unnecessary features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Feature Engineering pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEng(BaseEstimator, TransformerMixin):\n",
    "\tdef __init__(self):\n",
    "\t\tself.membership_order = ['No Membership', 'Basic Membership', 'Silver Membership',\n",
    "\t\t\t\t\t\t\t\t 'Gold Membership', 'Platinum Membership', 'Premium Membership']\n",
    "\t\tself.positive_feedback = ['Products always in Stock', 'Quality Customer Care', 'Reasonable Price', 'User Friendly Website']\n",
    "\t\tself.negative_feedback = ['Poor Website', 'Poor Customer Service', 'Poor Product Quality', 'Too many ads']\n",
    "\n",
    "\tdef get_sentiment(self, feedback):\n",
    "\t\tif feedback in self.positive_feedback:\n",
    "\t\t\treturn 1\n",
    "\t\telif feedback in self.negative_feedback:\n",
    "\t\t\treturn -1\n",
    "\t\telse:\n",
    "\t\t\treturn 0\n",
    "\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, X):\n",
    "\t\tX = X.copy()\n",
    "\t\t\n",
    "\t\tX['membership_category'] = pd.Categorical( X['membership_category'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  categories=self.membership_order, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  ordered=True).codes\n",
    "\t\t\n",
    "\t\tX['feedback'] = X['feedback'].apply(self.get_sentiment)\n",
    "\t\t\n",
    "\t\treturn X\n",
    "\t\n",
    "\tdef fit_transform(self, X, y=None):\n",
    "\t\tX_transformed = self.transform(X)\n",
    "\t\tself.feature_names_out_ = X_transformed.columns\n",
    "\t\treturn X_transformed\n",
    "\t\n",
    "\tdef get_feature_names_out(self, input_features=None):\n",
    "\t\treturn self.feature_names_out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_subset = X_train[['membership_category', 'feedback']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureEng_trans = FeatureEng()\n",
    "X_train_preprocessed_subset = featureEng_trans.fit_transform(X_train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save preprocessed data (ready for modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_preprocessed = pd.concat([X_train_preprocessed_subset, y_train], axis=1)\n",
    "train_split_preprocessed.to_csv('train_split_preprocessed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
