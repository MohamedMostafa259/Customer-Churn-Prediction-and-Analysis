{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "All hypothesis tests in this notebook will use a significance level ($\\alpha$) of **0.05**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant\n",
    "ALPHA = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "from scipy.stats import levene\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'https://raw.githubusercontent.com/MohamedMostafa259/Customer-Churn-Prediction-and-Analysis/main/Milestone2_FeatureEng_AdvancedAnalysis/data/train_basicFeatureEng.csv')\n",
    "train.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.hist(bins=50, figsize=(10, 7))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "It's clear that most of the distributions are not normal, but thanks to Central Limit Theorem we still can use parametric tests as our dataset size is large!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### **Oneway ANOVA test** (numerical features vs. churn risk score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "ANOVA test assumes homogeneity of variance, which means that the variance of the feature should be roughly equal across all groups (all churn risk scores). So, let's test that for each numeric column we have using the levene test.\n",
    "\n",
    "-\tIf p > 0.05: no significant difference in variance → safe to proceed with ANOVA\n",
    "\n",
    "-\tp <= 0.05: variances significantly differ → use Welch’s correction that doesn't assume equal variance and adjusts the degree of freedom based on that or use non-parametric tests instead (e.g, kruskal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_col in train_copy.select_dtypes('number').columns[:6]:\n",
    "\tgroups = [group[num_col] for _, group in train_copy.groupby('churn_risk_score')]\n",
    "\tstat, p_val = levene(*groups)\n",
    "\tprint(f'{num_col}: p-value = {p_val:.4f}', end=f', ')\n",
    "\tif p_val > ALPHA:\n",
    "\t\tprint('equal variances')\n",
    "\telse:\n",
    "\t\tprint('unequal variances - WARNING', '!!'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Most columns don't meet the assumption of equal variances, so we'll apply ANOVA to `'age'` and `'days_since_last_login'` columns only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "**Research Question:** Is there a significant difference in the mean value of `'age'` and `'days_since_last_login'` numerical features across the churn risk scores?\n",
    "\n",
    "$H_0$: The mean of `'age'` and `'days_since_last_login'` numerical features is the same across all churn risk score levels.\n",
    "\n",
    "$H_1$: At least one group (churn risk score) has a different mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_col in train_copy.select_dtypes('number').columns[:2]:\n",
    "\tprint('-'*30, f'\\n{num_col} vs. churn_risk_score')\n",
    "\tprint(pg.anova(data=train_copy, dv=num_col, between='churn_risk_score'), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "For `'days_since_last_login'`, let's apply a further analysis (post-hoc test) to investigate which two or more categories have significant differences between their means.\n",
    "\n",
    "**N.B.** The **family-wise error rate** (FWER) refers to the risk of making one or more **Type I errors** (false positives) when performing multiple statistical tests within a set of comparisons, and methods like **Bonferroni** correction are applied to control this error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.pairwise_tests(data=train_copy, dv='days_since_last_login', between='churn_risk_score', padjust='bonf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "sns.barplot(data=train_copy, x='churn_risk_score', y='days_since_last_login')\n",
    "plt.show()\n",
    "\n",
    "pg.pairwise_tests(data=train_copy, dv='days_since_last_login', between='churn_risk_score', padjust='bonf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = train_copy.select_dtypes('number').columns[:6].tolist()\n",
    "X = train_copy[num_features]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_results = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# add t-SNE components to the DataFrame\n",
    "train_copy['tsne-2d-one'] = tsne_results[:, 0]\n",
    "train_copy['tsne-2d-two'] = tsne_results[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy['churn_risk_level'] = train_copy['churn_risk_score'].apply(lambda x: 'low' if x <= 2 else 'high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=train_copy, x='tsne-2d-one', y='tsne-2d-two', hue='churn_risk_level', alpha=0.3)\n",
    "plt.title('t-SNE Visualization of Churn Risk Levels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "- For `age`, the p-value > 0.05, so we **fail to reject** $H_0$. There's no significant difference in average age across churn risk scores.\n",
    "\n",
    "- For `days_since_last_login`, the p-value are all less than 0.05, meaning we **reject** $H_0$. There's statistically significant differences in mean values across churn risk scores.\n",
    "\n",
    "\t-\tThe pairwise comparisons between churn risk scores and t_SNE visualization of churn risk levels reveal two distinct customer groups based on their login behavior. Customers with risk scores of 1 and 2 show no significant difference in their days_since_last_login, indicating similar recent activity levels and suggesting a low likelihood of churn. In contrast, customers with risk scores of 3, 4, and 5 also show no significant differences among themselves but show a clear and statistically significant difference from those in the 1 and 2 group. This separation implies that risk scores 3, 4, and 5 are associated with customers who have not logged in for longer periods, pointing to a higher risk of churn. Therefore, the churn risk score effectively distinguishes between low-risk (scores 1 and 2) and high-risk (scores 3, 4, and 5) customer segments based on login activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### **Two-sample ttest**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "We'll apply ttest with Welch's correction to the columns with no equal variance (`avg_time_spent`, `avg_transaction_value`, `avg_frequency_login_days`, and `points_in_wallet`), but now we'll group by the `churn_risk_level`column created below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "**Research Question:** Do behavioral time spent, transaction value, login frequency, and loyalty points significantly differ between customers with high and low churn risk?\n",
    "\n",
    "$H_0$: There is no difference in the mean values of the `'avg_time_spent'`, `'avg_transaction_value'`, `'avg_frequency_login_days'`, and `'points_in_wallet'` columns between customers with high and low churn risk\n",
    "\n",
    "$H_1$: There is a significant difference in at least one of these features between customers with high and low churn risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.select_dtypes('number').columns[2:6].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_col in train_copy.select_dtypes('number').columns[2:6]:\n",
    "\tprint('-'*30, f'\\n{num_col} vs. churn_risk_level')\n",
    "\tprint(pg.ttest(\n",
    "\tx=train_copy[train_copy['churn_risk_level'] == 'high'][num_col],\n",
    "\ty=train_copy[train_copy['churn_risk_level'] == 'low'][num_col],\n",
    "\talternative='two-sided', \n",
    "\tcorrection=True\n",
    "\t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()  # make it easier to index with a single loop\n",
    "\n",
    "for idx, col in enumerate(train_copy.select_dtypes('number').columns[2:6]):\n",
    "\tsns.boxplot(data=train_copy, hue='churn_risk_level', x=col, ax=axes[idx])\n",
    "\taxes[idx].set_title(f'{col} vs churn_risk_level')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "**Conclusion:** \n",
    "\n",
    "- For all columns: `'avg_time_spent'`, `'avg_transaction_value'`, `'avg_frequency_login_days'`, and `'points_in_wallet'`, the p-value < 0.05, so we **reject** $H_0$ for all of them and conclude that there's no significant difference in their means across churn risk scores.\n",
    "\n",
    "- High churn risk is consistently linked to lower engagement metrics (time spent, transaction value, loyalty points) but higher login frequency, implying these customers may be encountering service frustrations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### **Chi-squared test of independence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.select_dtypes(object).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted from the FeatureEng custom transformer in the FeatureEngineering notebook in milestone 2\n",
    "positive_feedback = ['Products always in Stock', 'Quality Customer Care', 'Reasonable Price', 'User Friendly Website']\n",
    "negative_feedback = ['Poor Website', 'Poor Customer Service', 'Poor Product Quality', 'Too many ads']\n",
    "\n",
    "def get_sentiment(feedback):\n",
    "\tif feedback in positive_feedback:\n",
    "\t\treturn 'positive'\n",
    "\telif feedback in negative_feedback:\n",
    "\t\treturn 'negative'\n",
    "\telse:\n",
    "\t\treturn 'neutral'\n",
    "\t\n",
    "train_copy['feedback'] = train_copy['feedback'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_cols = []\n",
    "for cat_col in train_copy.select_dtypes(object).columns:\n",
    "\tif cat_col in ['joining_date', 'last_visit_time', 'churn_risk_level']:\n",
    "\t\tcontinue\n",
    "\tprint('-'*30, f'\\n{cat_col} vs. churn_risk_level')\n",
    "\texcepted, observed, stats = pg.chi2_independence(data=train_copy, x='churn_risk_level', y=cat_col)\n",
    "\tprint(stats[stats['test'] == 'pearson'], '\\n')\n",
    "\n",
    "\tif stats[stats['test'] == 'pearson'].loc[0, 'pval'] <= ALPHA:\n",
    "\t\tassociated_cols.append(cat_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(associated_cols))\n",
    "associated_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "membership_order = ['No Membership', 'Basic Membership', 'Silver Membership',\n",
    "\t\t\t\t\t'Gold Membership', 'Platinum Membership', 'Premium Membership']\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()  # make it easier to index with a single loop\n",
    "\n",
    "for idx, associated_col in enumerate(associated_cols):\n",
    "\tprops = train_copy.groupby('churn_risk_level')[associated_col].value_counts(normalize=True).unstack()\n",
    "\n",
    "\tif associated_col == 'membership_category':\n",
    "\t\tprops = props[membership_order]\n",
    "\n",
    "\tprops.plot(kind='bar', stacked=True, rot=0, ax=axes[idx])\n",
    "\taxes[idx].set_title(f'{associated_col} vs churn_risk_level')\n",
    "\taxes[idx].set_ylabel('Proportion')\n",
    "\t# `bbox_to_anchor=(1.05, 1)` places the legend outside the right of the plot\n",
    "\taxes[idx].legend(\n",
    "\t\ttitle=associated_col,\n",
    "\t\tbbox_to_anchor=(1.05, 1),\n",
    "\t\tloc='upper left',\n",
    "\t\tborderaxespad=0.\n",
    "\t)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "**Conclusion**:\n",
    "\n",
    "Among all the associated columns, `membership_category` and `feedback` show the strongest relationship with the `churn_risk_level`.\n",
    "\n",
    "-\t**Membership Category**:\n",
    "\n",
    "\t-\tNotably, 0% of customers with low churn risk hold either ‘No Membership’ or ‘Basic Membership’, while these two categories make up a large portion of customers with high churn risk (around 50%).\n",
    "\t\n",
    "\t-\tIn contrast, Silver, Gold, Platinum, and Premium Memberships are more common among customers with low churn risk.\n",
    "\t\n",
    "\t-\tThis suggests that higher-tier memberships are associated with lower churn risk, possibly because such customers are more engaged or receive more value from the service.\n",
    "\n",
    "-\t**Feedback**:\n",
    "\n",
    "\t-\t100% of customers with low churn risk left positive feedback, with 0% negative or neutral feedback, indicating a strong satisfaction level.\n",
    "\t\n",
    "\t-\tOn the other hand, customers with high churn risk provided no positive feedback at all—80% gave negative feedback and the remaining 20% gave neutral feedback.\n",
    "\t\n",
    "\t-\tThis highlights a clear correlation between customer satisfaction and churn risk. Dissatisfied customers (those giving negative or neutral feedback) are far more likely to fall into the high churn risk category.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7371309,
     "sourceId": 11742428,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
