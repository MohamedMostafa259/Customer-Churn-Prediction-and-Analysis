{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # verstack requirements.txt file:\n",
    "\n",
    "# !pip install \\\n",
    "#     \"numpy>=1.26.4,<=2.1.1\" \\\n",
    "#     \"pandas==2.2.2\" \\\n",
    "#     \"scikit-learn>=1.3.2,<=1.5.1\" \\\n",
    "#     \"lightgbm>=4.4.0,<=4.5.0\" \\\n",
    "#     \"optuna>=3.5.0,<=4.0.0\" \\\n",
    "#     \"optuna-integration>=3.2.0,<=4.0.0\" \\\n",
    "#     \"plotly>=5.11.0,<=5.24.0\" \\\n",
    "#     \"matplotlib==3.9.2\" \\\n",
    "#     \"seaborn==0.13.2\" \\\n",
    "#     \"python-dateutil==2.9.0\" \\\n",
    "#     \"holidays==0.56\" \\\n",
    "#     \"mlxtend==0.23.1\" \\\n",
    "#     \"category_encoders>=2.5.1,<=2.6.3\" \\\n",
    "#     \"verstack\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate, train_test_split, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression # Baseline\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.svm import SVC \n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import joblib\n",
    "from verstack import NaNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Load preprocessed data (ready for modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed = pd.read_csv('https://raw.githubusercontent.com/MohamedMostafa259/Customer-Churn-Prediction-and-Analysis/refs/heads/main/Milestone2_FeatureEng_AdvancedAnalysis/data/train_split_preprocessed.csv')\n",
    "train_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = train_preprocessed.drop('churn_risk_score', axis=1)\n",
    "y_train = train_preprocessed['churn_risk_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Selecting promising models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'SVC': SVC(),\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'CatBoost': CatBoostClassifier(verbose=0),\n",
    "    'LightGBM': LGBMClassifier(verbose=0)\n",
    "}\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"CV on {name}\")\n",
    "    cv_results = cross_validate(\n",
    "        model,\n",
    "        X_train_preprocessed,\n",
    "# target labels (y_train_split) starts from '1': [1, 2, 3, 4, 5], \n",
    "# but XGBoost expects them to start from 0, like [0, 1, 2, 3, 4].\n",
    "        y_train - 1,\n",
    "# don't forget to add one in the prediction time:\n",
    "# y_pred = xgb_clf.predict(X_test_preprocessed) + 1\n",
    "        cv=4,\n",
    "        scoring='accuracy',\n",
    "        return_train_score=True\n",
    "    )\n",
    "    print(f\"Train Accuracy: {cv_results['train_score'].mean():.4f} (+/- {cv_results['train_score'].std():.4f})\")\n",
    "    print(f\"Validation Accuracy: {cv_results['test_score'].mean():.4f} (+/- {cv_results['test_score'].std():.4f})\")\n",
    "    print('-'*30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "It's clear that XGBoost, CatBoost, LightGBM are the best models. So, we will take them to next step, which hyperparamater tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('model', LogisticRegression()) # placeholder will be replaced by the best model\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'model': [XGBClassifier(verbosity=0, random_state=42)],\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1],\n",
    "        'model__max_depth': [6, 10, 13],\n",
    "        'model__subsample': [0.5, 0.75, 1],\n",
    "    },\n",
    "    {\n",
    "        # `bootstrap_type='Bernoulli'` is required for `subsample` parameter to work\n",
    "        'model': [CatBoostClassifier(verbose=0, random_state=42, bootstrap_type='Bernoulli')],\n",
    "        'model__iterations': [100, 200],  # CatBoost uses iterations instead of n_estimators\n",
    "        'model__learning_rate': [0.01, 0.1],\n",
    "        'model__depth': [6, 10, 13],  # CatBoost uses depth instead of max_depth\n",
    "        'model__subsample': [0.5, 0.75, 1],\n",
    "    },\n",
    "    {\n",
    "        'model': [LGBMClassifier(verbose=-1, random_state=42)],\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1],\n",
    "        'model__max_depth': [6, 10, 13],\n",
    "        'model__subsample': [0.5, 0.75, 1],\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = RandomizedSearchCV(pipe, param_distributions=param_grid, cv=4, scoring='accuracy', \n",
    "                    verbose=1, return_train_score=True, n_iter=60, n_jobs=-1, \n",
    "                    error_score='raise', random_state=42)\n",
    "\n",
    "grid.fit(X_train_preprocessed, y_train-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_df = pd.DataFrame(grid.cv_results_).sort_values('mean_test_score', ascending=False)\n",
    "print(cv_results_df.shape)\n",
    "cols_to_show = [ \"rank_test_score\", \"mean_test_score\", \"std_test_score\",\n",
    "                \"mean_train_score\", \"std_train_score\", \"param_model\", \n",
    "                \"param_model__subsample\", \"param_model__n_estimators\",\n",
    "                \"param_model__max_depth\", \"param_model__learning_rate\"\n",
    "]\n",
    "cv_results_df[cols_to_show].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_df[(cv_results_df['param_model__subsample']==0.750) & (cv_results_df['param_model__n_estimators']==100) &\\\n",
    "                (cv_results_df['param_model__max_depth']==6) & (cv_results_df['param_model__learning_rate']==0.01)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "The best model was `XGBClassifier` with these parameters:\n",
    "- 'subsample': 0.75,\n",
    "- 'n_estimators': 200,\n",
    "- 'max_depth': 6,\n",
    "- 'learning_rate': 0.01\n",
    "\n",
    "Let's see what is the performance of the same model with 'n_estimators'=100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(verbosity=0, random_state=42, n_estimators=100, \n",
    "                       learning_rate=0.01, max_depth=6, subsample=0.75)\n",
    "xgb_cv_results = cross_validate(xgb_clf, X_train_preprocessed, y_train-1, \n",
    "                                cv=4, scoring='accuracy', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"XGB Train Accuracy: {xgb_cv_results['train_score'].mean():.4f} (+/- {xgb_cv_results['train_score'].std():.4f})\")\n",
    "print(f\"XGB Validation Accuracy: {xgb_cv_results['test_score'].mean():.4f} (+/- {xgb_cv_results['test_score'].std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nBest model from cross validation:')\n",
    "cv_results_df[['mean_train_score', 'mean_test_score']].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "It seems that the best model XGBClassifier, resulted from the Randomized Cross Validation above, performs better (or equivalent) when having less number of estimators: n_estimators=100 !!\n",
    "\n",
    "So, let's stick with this simpler version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### Merging the selected ml model with the previous pipelines & transformers into one pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `DataCleaner` and `NaNImputerWrapper` are needed for `cleaning_pipeline`\n",
    "from custom_transformers import DataCleaner, NaNImputerWrapper, FeatureEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_cleaned = pd.read_csv('https://raw.githubusercontent.com/MohamedMostafa259/Customer-Churn-Prediction-and-Analysis/main/Milestone1_DataCollection_EDA_DataCleaning/data/train_split_cleaned.csv')[['membership_category', 'feedback', 'points_in_wallet', 'churn_risk_score']]\n",
    "val_split_cleaned = pd.read_csv('https://raw.githubusercontent.com/MohamedMostafa259/Customer-Churn-Prediction-and-Analysis/main/Milestone1_DataCollection_EDA_DataCleaning/data/validation_split_cleaned.csv')[['membership_category', 'feedback', 'points_in_wallet', 'churn_risk_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_split = train_split_cleaned.drop('churn_risk_score', axis=1)\n",
    "y_train_split = train_split_cleaned['churn_risk_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_split = val_split_cleaned.drop('churn_risk_score', axis=1)\n",
    "y_val_split = val_split_cleaned['churn_risk_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ../ means go up one dir\n",
    "cleaning_pipeline = joblib.load(\"../../Milestone1_DataCollection_EDA_DataCleaning/pipelines/cleaning_pipeline.joblib\")\n",
    "cleaning_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('cleaning_pipeline', cleaning_pipeline),  \n",
    "    ('feature_engineering', FeatureEng()), \n",
    "\t('model', xgb_clf)\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train_split, y_train_split-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = pipeline.predict(X_train_split) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_accuracy:', accuracy_score(y_train_split, y_train_pred), '\\n')\n",
    "print('train_classification_report:\\n', classification_report(y_train_split, y_train_pred), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = pipeline.predict(X_val_split) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_accuracy:', accuracy_score(y_val_split, y_val_pred), '\\n')\n",
    "print('test_classification_report:\\n', classification_report(y_val_split, y_val_pred), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Summary of the model results\n",
    "\n",
    "* **Overall test accuracy**: **0.788** → The model correctly predicts the class **\\~79% of the time** across the entire test set.\n",
    "\n",
    "* **F1-score (macro avg) = F1-score (weighted avg) = 0.77** → The model performs well overall despite imbalanced data. This score is equal to the score of the 1st winner of the competition made on this dataset. [He built a model with **0.77** f1_macro](https://www.hackerearth.com/challenges/new/competitive/hackerearth-machine-learning-challenge-predict-customer-churn/#:~:text=Machine%20Learning%20practice-,Winners,-Adarsh%20Wase).\n",
    "\n",
    "    - The [**evaluation metric**](https://www.hackerearth.com/problem/machine-learning/predict-the-churn-risk-rate-11-fb7a760d/#:~:text=score%20%3D%20100%20x%20metrics.f1_score(actual%2C%20predicted%2C%20average%3D%22macro%22)) for the HackerEarth Machine Learning Challenge: \"How NOT to Lose a Customer in 10 Days\" was based on the macro-averaged F1 score. Specifically, the final score was calculated as: **Final Score = 100 × F1_macro**\n",
    "\n",
    "---\n",
    "\n",
    "**One critical limitation of the model:**\n",
    "\n",
    "It has low recall (0.43) on class 4, leading to a low f1-score (0.57) on that class as well. \n",
    "\n",
    "As a result, over half of actual class 4 samples are misclassified. That’s unacceptable for a high-risk churn group.\n",
    "\n",
    "**Solution** may be:\n",
    "\n",
    "* increasing the predicted probability of class 4 \n",
    "\n",
    "* More examples \n",
    "\n",
    "* Better features focusing on separating class 4\n",
    "\n",
    "<br>\n",
    "\n",
    "**Another limitation** (less dangerous):\n",
    "\n",
    "It has low recall (0.59) on class 2; however, we care more about addressing higher risk scores (recall of 3, 4, and 5 scores). But, we should take this into consideration as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_train = xgb_clf.predict_proba(X_train_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis_df = pd.DataFrame(np.c_[y_proba_train, np.array(y_train_split), np.array(y_train_pred)], \n",
    "                                 columns=['class1_prob', 'class2_prob', 'class3_prob', 'class4_prob', \n",
    "                                          'class5_prob', 'y_train', 'y_pred']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Erros Analysis for class 4 samples that are misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis_df.loc[error_analysis_df['y_train']==4, ['y_train', 'y_pred']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "More than half of the class 4 samples are misclassified, mostly as 5.0, followed by 3.0. This explains the terrible recall (0.43) for class 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis_df.loc[(error_analysis_df['y_train']==4) & (error_analysis_df['y_pred']!=4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### Erros Analysis for class 2 samples that are misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis_df.loc[error_analysis_df['y_train']==2, ['y_train', 'y_pred']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "The model is confusing class 2 with class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis_df.loc[(error_analysis_df['y_train']==2) & (error_analysis_df['y_pred']!=2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "These mistakes are very logical and supports our conclusion using ANOVA and t-SNE when we found that these multiclass classification problem can designed as binary classification by considering two classes low risk (scores 1 and 2) and high risk (scores 3, 4, and 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Generally in misclassified 4 and 2 class samples, the differences between the probability of the true class and the predicted class are very close. \n",
    "\n",
    "So, we will increase the probability of both classes 4 and 2 a little bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Create custom model with adjusted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustedProbClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model, thresholds):\n",
    "        self.model = model\n",
    "        self.thresholds = thresholds\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_proba = self.predict_proba(X)\n",
    "        preds = []\n",
    "        for probs in y_proba:\n",
    "            predicted_class = np.argmax(probs / np.array(self.thresholds)) + 1\n",
    "            preds.append(predicted_class)\n",
    "        return np.array(preds)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### Increasing the probability of class 2 and 4 to increase their recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_grid = []\n",
    "thresholds_to_try = [0.35, 0.37, 0.39, 0.41, 0.43]\n",
    "for class2_threshold in thresholds_to_try:\n",
    "    for class4_threshold in thresholds_to_try:\n",
    "        thresholds_grid.append([0.5, class2_threshold, 0.5, class4_threshold, 0.5])\n",
    "\n",
    "for thresholds in thresholds_grid:\n",
    "    adjusted_prob_clf = AdjustedProbClassifier(xgb_clf, thresholds)\n",
    "    y_pred = cross_val_predict(adjusted_prob_clf, X_train_preprocessed, y_train-1, \n",
    "                               cv=4, n_jobs=-1)\n",
    "    print(f\"Thresholds: {thresholds}\")\n",
    "    print(f\"Classification Report:\\n{classification_report(y_train, y_pred)}\")\n",
    "    print('-' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "It seems that the best thresholds combination is: [0.5, 0.43, 0.5, 0.43, 0.5] (last one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_adjusted_prob = Pipeline([\n",
    "    ('cleaning_pipeline', cleaning_pipeline),  \n",
    "    ('feature_engineering', FeatureEng()), \n",
    "\t('model', adjusted_prob_clf)\n",
    "])\n",
    "\n",
    "pipeline_adjusted_prob.fit(X_train_split, y_train_split-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_adjusted_prob = pipeline_adjusted_prob.predict(X_train_split) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_accuracy:', accuracy_score(y_train_split, y_train_pred_adjusted_prob), '\\n')\n",
    "print('train_classification_report:\\n', classification_report(y_train_split, y_train_pred_adjusted_prob), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred_adjusted_prob = pipeline_adjusted_prob.predict(X_val_split) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_accuracy:', accuracy_score(y_val_split, y_val_pred_adjusted_prob), '\\n')\n",
    "print('test_classification_report:\\n', classification_report(y_val_split, y_val_pred_adjusted_prob), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "This model with adjusted probabities seems to do slightly better in terms of f1_score and has more balanced precision and recall values. \n",
    "\n",
    "It may be better than the score of the 1st winner of the competition made on this dataset. [He built a model with **0.77** f1_macro](https://www.hackerearth.com/challenges/new/competitive/hackerearth-machine-learning-challenge-predict-customer-churn/#:~:text=Machine%20Learning%20practice-,Winners,-Adarsh%20Wase); however, my model has f1_macro of **0.78**\n",
    "\n",
    "So, let's use this model for deployment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "#### Is tuning probabilities useless? (in the context of business goals, not competitions)\n",
    "\n",
    "I think that tuning probabilities **in this case** is not that useful because when the model missclassifies class 4 samples, it predicts them as class 3 or class 5 (all of them belong to the same category: high risk score) and when it misclassifies class 2 samples, it predicts them as class 1, which is also a low risk score. The point is that the model doesn't mix high scores with low scores. The model's misclassifications are \"within-category\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "#### Group-level accuracy (validation of the argument in the previous cell: whether tuning probabilities useless) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'high risk score' is encoded as 1 and 'low risk score' is encoded as 0\n",
    "y_train_binary = y_train.apply(lambda x: 1 if x >= 3 else 0)\n",
    "y_train_binary.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf_binary = XGBClassifier(verbosity=0, random_state=42, n_estimators=100, \n",
    "                       learning_rate=0.01, max_depth=6, subsample=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_binaryClassification = cross_validate(xgb_clf_binary, X_train_preprocessed, y_train_binary, \n",
    "                                                cv=4, scoring='accuracy', return_train_score=True)\n",
    "print(f\"Train Accuracy (Binary Classification): {cv_results_binaryClassification['train_score'].mean():.4f} (+/- {cv_results_binaryClassification['train_score'].std():.4f})\")\n",
    "print(f\"Validation Accuracy (Binary Classification): {cv_results_binaryClassification['test_score'].mean():.4f} (+/- {cv_results_binaryClassification['test_score'].std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_binary = Pipeline([\n",
    "    ('cleaning_pipeline', cleaning_pipeline),  \n",
    "    ('feature_engineering', FeatureEng()), \n",
    "\t('model', xgb_clf_binary)\n",
    "])\n",
    "\n",
    "pipeline_binary.fit(X_train_split, y_train_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred_binary = pipeline_binary.predict(X_val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_binary = y_val_split.apply(lambda x: 1 if x >= 3 else 0)\n",
    "y_val_binary.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Accuracy (Binary Classification):', accuracy_score(y_val_binary, y_val_pred_binary), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "The argument that we should not focus that much on tuning probabilities in this multiclassification problem seems to be true only if the business will make the same decisions for high-risk scores (3, 4, and 5) and same decisions for low-risk scores (1 and 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "### Saving the final ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(pipeline, '/kaggle/working/full_xgb_pipeline.joblib')\n",
    "joblib.dump(pipeline_adjusted_prob, '/kaggle/working/full_xgb_pipeline_adjusted_prob.joblib')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7371309,
     "sourceId": 11742428,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 354784,
     "modelInstanceId": 333788,
     "sourceId": 408540,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 356101,
     "modelInstanceId": 335083,
     "sourceId": 410350,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
