{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook order\n",
    "\n",
    "This notebook comes after the EDA notebook which investigate what to do in the data cleaning process.\n",
    "\n",
    "You can find EDA.ipynb in:  Milestone1_DataCollection_EDA_DataCleaning\\notebooks\\EDA.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from verstack import NaNImputer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>Name</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>security_no</th>\n",
       "      <th>region_category</th>\n",
       "      <th>membership_category</th>\n",
       "      <th>joining_date</th>\n",
       "      <th>joined_through_referral</th>\n",
       "      <th>referral_id</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_time_spent</th>\n",
       "      <th>avg_transaction_value</th>\n",
       "      <th>avg_frequency_login_days</th>\n",
       "      <th>points_in_wallet</th>\n",
       "      <th>used_special_discount</th>\n",
       "      <th>offer_application_preference</th>\n",
       "      <th>past_complaint</th>\n",
       "      <th>complaint_status</th>\n",
       "      <th>feedback</th>\n",
       "      <th>churn_risk_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9550</th>\n",
       "      <td>fffe43004900440033003200320035003600</td>\n",
       "      <td>Tobias Liebold</td>\n",
       "      <td>24</td>\n",
       "      <td>F</td>\n",
       "      <td>I4AYTC2</td>\n",
       "      <td>City</td>\n",
       "      <td>Premium Membership</td>\n",
       "      <td>2015-04-22</td>\n",
       "      <td>No</td>\n",
       "      <td>xxxxxxxx</td>\n",
       "      <td>...</td>\n",
       "      <td>101.50</td>\n",
       "      <td>32593.20</td>\n",
       "      <td>15.0</td>\n",
       "      <td>801.18</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>Products always in Stock</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7112</th>\n",
       "      <td>fffe43004900440032003200350035003400</td>\n",
       "      <td>Patrick Kizer</td>\n",
       "      <td>53</td>\n",
       "      <td>F</td>\n",
       "      <td>WV0LB6W</td>\n",
       "      <td>Town</td>\n",
       "      <td>Silver Membership</td>\n",
       "      <td>2016-01-19</td>\n",
       "      <td>No</td>\n",
       "      <td>xxxxxxxx</td>\n",
       "      <td>...</td>\n",
       "      <td>324.61</td>\n",
       "      <td>39155.49</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>No reason specified</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9545</th>\n",
       "      <td>fffe43004900440031003000380038003300</td>\n",
       "      <td>Annamaria Freese</td>\n",
       "      <td>53</td>\n",
       "      <td>F</td>\n",
       "      <td>94O1F22</td>\n",
       "      <td>Town</td>\n",
       "      <td>No Membership</td>\n",
       "      <td>2016-02-07</td>\n",
       "      <td>Yes</td>\n",
       "      <td>CID19334</td>\n",
       "      <td>...</td>\n",
       "      <td>47.71</td>\n",
       "      <td>35434.17</td>\n",
       "      <td>12.0</td>\n",
       "      <td>675.17</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>Poor Product Quality</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10261</th>\n",
       "      <td>fffe43004900440034003200300031003800</td>\n",
       "      <td>Gilda Lundy</td>\n",
       "      <td>61</td>\n",
       "      <td>M</td>\n",
       "      <td>74WFG9K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gold Membership</td>\n",
       "      <td>2017-10-24</td>\n",
       "      <td>No</td>\n",
       "      <td>xxxxxxxx</td>\n",
       "      <td>...</td>\n",
       "      <td>451.66</td>\n",
       "      <td>30621.93</td>\n",
       "      <td>7.0</td>\n",
       "      <td>755.93</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Solved</td>\n",
       "      <td>Poor Product Quality</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9876</th>\n",
       "      <td>fffe43004900440034003100380030003300</td>\n",
       "      <td>Angla Alameda</td>\n",
       "      <td>46</td>\n",
       "      <td>F</td>\n",
       "      <td>249HVEX</td>\n",
       "      <td>Town</td>\n",
       "      <td>Premium Membership</td>\n",
       "      <td>2016-06-11</td>\n",
       "      <td>No</td>\n",
       "      <td>xxxxxxxx</td>\n",
       "      <td>...</td>\n",
       "      <td>266.68</td>\n",
       "      <td>50462.15</td>\n",
       "      <td>Error</td>\n",
       "      <td>806.67</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Solved</td>\n",
       "      <td>Products always in Stock</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                customer_id              Name  age gender  \\\n",
       "9550   fffe43004900440033003200320035003600    Tobias Liebold   24      F   \n",
       "7112   fffe43004900440032003200350035003400     Patrick Kizer   53      F   \n",
       "9545   fffe43004900440031003000380038003300  Annamaria Freese   53      F   \n",
       "10261  fffe43004900440034003200300031003800       Gilda Lundy   61      M   \n",
       "9876   fffe43004900440034003100380030003300     Angla Alameda   46      F   \n",
       "\n",
       "      security_no region_category membership_category joining_date  \\\n",
       "9550      I4AYTC2            City  Premium Membership   2015-04-22   \n",
       "7112      WV0LB6W            Town   Silver Membership   2016-01-19   \n",
       "9545      94O1F22            Town       No Membership   2016-02-07   \n",
       "10261     74WFG9K             NaN     Gold Membership   2017-10-24   \n",
       "9876      249HVEX            Town  Premium Membership   2016-06-11   \n",
       "\n",
       "      joined_through_referral referral_id  ... avg_time_spent  \\\n",
       "9550                       No    xxxxxxxx  ...         101.50   \n",
       "7112                       No    xxxxxxxx  ...         324.61   \n",
       "9545                      Yes    CID19334  ...          47.71   \n",
       "10261                      No    xxxxxxxx  ...         451.66   \n",
       "9876                       No    xxxxxxxx  ...         266.68   \n",
       "\n",
       "      avg_transaction_value avg_frequency_login_days points_in_wallet  \\\n",
       "9550               32593.20                     15.0           801.18   \n",
       "7112               39155.49                     21.0              NaN   \n",
       "9545               35434.17                     12.0           675.17   \n",
       "10261              30621.93                      7.0           755.93   \n",
       "9876               50462.15                    Error           806.67   \n",
       "\n",
       "       used_special_discount  offer_application_preference  past_complaint  \\\n",
       "9550                     Yes                            No              No   \n",
       "7112                      No                           Yes              No   \n",
       "9545                     Yes                            No              No   \n",
       "10261                    Yes                           Yes             Yes   \n",
       "9876                     Yes                           Yes             Yes   \n",
       "\n",
       "      complaint_status                  feedback churn_risk_score  \n",
       "9550    Not Applicable  Products always in Stock                1  \n",
       "7112    Not Applicable       No reason specified                3  \n",
       "9545    Not Applicable      Poor Product Quality                5  \n",
       "10261           Solved      Poor Product Quality                3  \n",
       "9876            Solved  Products always in Stock                1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"https://raw.githubusercontent.com/MohamedMostafa259/Customer-Churn-Prediction-and-Analysis/main/Data/train.csv\")\n",
    "train.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a copy for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy = train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set unknown categories to `Nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy = train_copy.replace(['?', 'Error'], np.nan)\n",
    "train_copy['avg_frequency_login_days'] = train_copy['avg_frequency_login_days'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling negative incorrect values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonnegative_cols = ['days_since_last_login', 'avg_time_spent', 'avg_frequency_login_days', 'points_in_wallet']\n",
    "for col in nonnegative_cols:\n",
    "\ttrain_copy.loc[train_copy[col] < 0, col] = np.nan  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping rows with NaNs in the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.loc[train_copy['churn_risk_score'] == -1, 'churn_risk_score'] = np.nan\n",
    "train_copy.dropna(subset=['churn_risk_score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>days_since_last_login</th>\n",
       "      <th>avg_time_spent</th>\n",
       "      <th>avg_transaction_value</th>\n",
       "      <th>avg_frequency_login_days</th>\n",
       "      <th>points_in_wallet</th>\n",
       "      <th>churn_risk_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35829.000000</td>\n",
       "      <td>33885.000000</td>\n",
       "      <td>34170.000000</td>\n",
       "      <td>35829.000000</td>\n",
       "      <td>31751.000000</td>\n",
       "      <td>32354.000000</td>\n",
       "      <td>35829.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>37.120266</td>\n",
       "      <td>12.751424</td>\n",
       "      <td>292.491498</td>\n",
       "      <td>29304.272306</td>\n",
       "      <td>16.523181</td>\n",
       "      <td>690.389359</td>\n",
       "      <td>3.608278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.865360</td>\n",
       "      <td>5.574802</td>\n",
       "      <td>331.518007</td>\n",
       "      <td>19484.565419</td>\n",
       "      <td>8.374922</td>\n",
       "      <td>186.791727</td>\n",
       "      <td>1.176426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.837399</td>\n",
       "      <td>800.460000</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>6.432208</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>71.390000</td>\n",
       "      <td>14194.650000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>617.152500</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>173.990000</td>\n",
       "      <td>27584.530000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>698.420000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>370.907500</td>\n",
       "      <td>40874.010000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>764.307500</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>3235.578521</td>\n",
       "      <td>99914.050000</td>\n",
       "      <td>73.061995</td>\n",
       "      <td>2069.069761</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age  days_since_last_login  avg_time_spent  \\\n",
       "count  35829.000000           33885.000000    34170.000000   \n",
       "mean      37.120266              12.751424      292.491498   \n",
       "std       15.865360               5.574802      331.518007   \n",
       "min       10.000000               1.000000        1.837399   \n",
       "25%       23.000000               9.000000       71.390000   \n",
       "50%       37.000000              13.000000      173.990000   \n",
       "75%       51.000000              17.000000      370.907500   \n",
       "max       64.000000              26.000000     3235.578521   \n",
       "\n",
       "       avg_transaction_value  avg_frequency_login_days  points_in_wallet  \\\n",
       "count           35829.000000              31751.000000      32354.000000   \n",
       "mean            29304.272306                 16.523181        690.389359   \n",
       "std             19484.565419                  8.374922        186.791727   \n",
       "min               800.460000                  0.009208          6.432208   \n",
       "25%             14194.650000                 10.000000        617.152500   \n",
       "50%             27584.530000                 16.000000        698.420000   \n",
       "75%             40874.010000                 23.000000        764.307500   \n",
       "max             99914.050000                 73.061995       2069.069761   \n",
       "\n",
       "       churn_risk_score  \n",
       "count      35829.000000  \n",
       "mean           3.608278  \n",
       "std            1.176426  \n",
       "min            1.000000  \n",
       "25%            3.000000  \n",
       "50%            4.000000  \n",
       "75%            5.000000  \n",
       "max            5.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_copy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecessary cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['customer_id', 'Name', 'security_no', 'referral_id']\n",
    "train_copy = train_copy.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter categorical columns in a list to use later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['region_category',\n",
       " 'medium_of_operation',\n",
       " 'internet_option',\n",
       " 'offer_application_preference',\n",
       " 'last_visit_time',\n",
       " 'gender',\n",
       " 'joined_through_referral',\n",
       " 'joining_date',\n",
       " 'past_complaint',\n",
       " 'complaint_status',\n",
       " 'used_special_discount',\n",
       " 'feedback',\n",
       " 'membership_category',\n",
       " 'preferred_offer_types']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# date_cols = [('date', 'date_format'), ...]\n",
    "date_cols = [('joining_date', '%Y-%m-%d'), ('last_visit_time', '%H:%M:%S')]\n",
    "\n",
    "cat_cols = list(set(train_copy.select_dtypes(include='object').columns) - set(date_cols))\n",
    "# last_visit_time → categories: morning & evening, ...\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                                0\n",
       "gender                             0\n",
       "region_category                 5263\n",
       "membership_category                0\n",
       "joining_date                       0\n",
       "joined_through_referral         5292\n",
       "preferred_offer_types            276\n",
       "medium_of_operation             5230\n",
       "internet_option                    0\n",
       "last_visit_time                    0\n",
       "days_since_last_login           1944\n",
       "avg_time_spent                  1659\n",
       "avg_transaction_value              0\n",
       "avg_frequency_login_days        4078\n",
       "points_in_wallet                3475\n",
       "used_special_discount              0\n",
       "offer_application_preference       0\n",
       "past_complaint                     0\n",
       "complaint_status                   0\n",
       "feedback                           0\n",
       "churn_risk_score                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_copy.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the cleaned train.csv file into train & validation splits (to avoid data leakage during imputation)\n",
    "\n",
    "Initially, the dataset only contained two files: `train.csv` and `test.csv`. However, the `test.csv` file didn't include any target labels, and since the HackerEarth competition had already ended, I couldn't use it to evaluate my model's performance.\n",
    "\n",
    "**Solution:**  \n",
    "To address this, I will manually split the original `train.csv` into two separate sets: a new `train_split.csv` and a `validation_split.csv`, using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, validation_split = train_test_split(train_copy, test_size=0.2, random_state=42, stratify=train_copy['churn_risk_score'])\n",
    "\n",
    "train_copy.to_csv('train_cleaned.csv', index=False)\n",
    "train_split.to_csv('train_split_cleaned.csv', index=False)\n",
    "validation_split.to_csv('validation_split_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Handling missing value approach**\n",
    "\n",
    "I will use `verstack.NaNImputer` as it uses a powerful model-based imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the whole cleaning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataCleaner Transformer\n",
    "\n",
    "This transformer:\n",
    "\n",
    "-\tdrops unwanted cols\n",
    "\n",
    "-\treplace unknown categories (e.g., '?') with `np.nan`\n",
    "\n",
    "-\thandle wrong negative values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner(BaseEstimator, TransformerMixin):\n",
    "\tdef __init__(self, cols_to_drop, nonnegative_cols):\n",
    "\t\tself.cols_to_drop = cols_to_drop\n",
    "\t\tself.nonnegative_cols = nonnegative_cols\n",
    "   \n",
    "\tdef fit(self, X, y=None):\n",
    "\t\treturn self\n",
    "\n",
    "\t# X is pd.DataFrame\n",
    "\tdef transform(self, X):\n",
    "\t\tX_copy = X.copy()\n",
    "\t\tX_copy.drop(columns=self.cols_to_drop, errors='ignore', inplace=True)\t\n",
    "\t\t\t\n",
    "\t\tX_copy.replace(['?', 'Error'], np.nan, inplace=True)\n",
    "\t\tX_copy['avg_frequency_login_days'] = X_copy['avg_frequency_login_days'].astype(float)\n",
    "\n",
    "\t\tfor col in self.nonnegative_cols:\n",
    "\t\t\tX_copy.loc[X_copy[col] < 0, col] = np.nan  \n",
    "\n",
    "\t\treturn X_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Wrapping `verstack.NaNImputer` into an custom transformer for compatibility with scikit-learn's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by the Adapter design pattern ;)\n",
    "class NaNImputerWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, train_sample_size=30_000, verbose=True):\n",
    "        self.train_sample_size = train_sample_size\n",
    "        self.verbose = verbose\n",
    "        self.imputer = NaNImputer(self.train_sample_size, self.verbose)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.imputer.impute(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating transformers into a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_train_y_train(df, y='churn_risk_score'):\n",
    "\tX_train = df.drop(columns=[y])\n",
    "\ty_train = df[y]\n",
    "\treturn X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing train_copy for advanced analysis and building dashboards\n",
    "X_train, y_train = get_X_train_y_train(train_copy)\n",
    "\n",
    "# imputing train_split for model development (we avoid imputing the validation set here to avoid data leakage)\n",
    "X_train_split, y_train_split = get_X_train_y_train(train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * Initiating NaNImputer.impute\n",
      "     . Dataset dimensions:\n",
      "     .. rows:         35829\n",
      "     .. columns:      20\n",
      "     .. mb in memory: 5.74\n",
      "     .. NaN cols num: 8\n",
      "\n",
      "   - Drop hopeless NaN cols\n",
      "\n",
      "   - Processing whole data for imputation\n",
      "     . Processed 10 cols; 8 to go\n",
      "\n",
      "   - Imputing single core 8 cols\n",
      "     . Imputed (multiclass) - 5263     NaN in region_category\n",
      "     . Imputed (  binary  ) - 5292     NaN in joined_through_referral\n",
      "     . Imputed (multiclass) - 276      NaN in preferred_offer_types\n",
      "     . Imputed (multiclass) - 5230     NaN in medium_of_operation\n",
      "     . Imputed (multiclass) - 1944     NaN in days_since_last_login\n",
      "     . Imputed (regression) - 1659     NaN in avg_time_spent\n",
      "     . Imputed (regression) - 4078     NaN in avg_frequency_login_days\n",
      "     . Imputed (regression) - 3475     NaN in points_in_wallet\n",
      "\n",
      "   - Missing values after imputation: 0\n",
      "\n",
      "Time elapsed for impute execution: 55.04399 seconds\n",
      "\n",
      " * Initiating NaNImputer.impute\n",
      "     . Dataset dimensions:\n",
      "     .. rows:         28663\n",
      "     .. columns:      20\n",
      "     .. mb in memory: 4.59\n",
      "     .. NaN cols num: 8\n",
      "\n",
      "   - Drop hopeless NaN cols\n",
      "\n",
      "   - Processing whole data for imputation\n",
      "     . Processed 10 cols; 8 to go\n",
      "\n",
      "   - Imputing single core 8 cols\n",
      "     . Imputed (multiclass) - 4170     NaN in region_category\n",
      "     . Imputed (  binary  ) - 4231     NaN in joined_through_referral\n",
      "     . Imputed (multiclass) - 224      NaN in preferred_offer_types\n",
      "     . Imputed (multiclass) - 4211     NaN in medium_of_operation\n",
      "     . Imputed (multiclass) - 1573     NaN in days_since_last_login\n",
      "     . Imputed (regression) - 1310     NaN in avg_time_spent\n",
      "     . Imputed (regression) - 3248     NaN in avg_frequency_login_days\n",
      "     . Imputed (regression) - 2769     NaN in points_in_wallet\n",
      "\n",
      "   - Missing values after imputation: 0\n",
      "\n",
      "Time elapsed for impute execution: 42.68274 seconds\n"
     ]
    }
   ],
   "source": [
    "cleaning_pipeline = Pipeline([\n",
    "    ('dataCleaner', DataCleaner(cols_to_drop, nonnegative_cols)), \n",
    "    ('imputer', NaNImputerWrapper(train_sample_size=train_split.shape[0]))\n",
    "])\n",
    "\n",
    "X_train_cleaned_imputed = cleaning_pipeline.fit_transform(X_train)\n",
    "X_train_split_cleaned_imputed = cleaning_pipeline.fit_transform(X_train_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(X, y, name, extension='.csv'):\n",
    "\tdf_cleaned_imputed = pd.concat([X, y], axis=1)\n",
    "\tdf_cleaned_imputed.to_csv(name + extension, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(X_train_cleaned_imputed, y_train, 'train_cleaned_imputed')\n",
    "\n",
    "save_df(X_train_split_cleaned_imputed, y_train_split, 'train_split_cleaned_imputed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save `cleaning_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cleaning_pipeline.joblib']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(cleaning_pipeline, 'cleaning_pipeline.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
