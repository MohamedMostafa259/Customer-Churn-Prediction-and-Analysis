{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook order\n",
    "\n",
    "This notebook comes after the EDA notebook which investigate what to do in the data cleaning process.\n",
    "\n",
    "You can find EDA.ipynb in:  Milestone1_DataCollection_EDA_DataCleaning\\notebooks\\EDA.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from verstack import NaNImputer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"https://raw.githubusercontent.com/MohamedMostafa259/Customer-Churn-Prediction-and-Analysis/main/Data/train.csv\")\n",
    "train.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a copy for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy = train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set unknown categories to `Nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy = train_copy.replace(['?', 'Error'], np.nan)\n",
    "train_copy['avg_frequency_login_days'] = train_copy['avg_frequency_login_days'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling negative incorrect values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonnegative_cols = ['days_since_last_login', 'avg_time_spent', 'avg_frequency_login_days', 'points_in_wallet']\n",
    "for col in nonnegative_cols:\n",
    "\ttrain_copy.loc[train_copy[col] < 0, col] = np.nan  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping rows with NaNs in the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.loc[train_copy['churn_risk_score'] == -1, 'churn_risk_score'] = np.nan\n",
    "train_copy.dropna(subset=['churn_risk_score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecessary cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['customer_id', 'Name', 'security_no', 'referral_id']\n",
    "train_copy = train_copy.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter categorical columns in a list to use later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_cols = [('date', 'date_format'), ...]\n",
    "date_cols = [('joining_date', '%Y-%m-%d'), ('last_visit_time', '%H:%M:%S')]\n",
    "\n",
    "cat_cols = list(set(train_copy.select_dtypes(include='object').columns) - set(date_cols))\n",
    "# last_visit_time â†’ categories: morning & evening, ...\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the cleaned train.csv file into train & validation splits (to avoid data leakage during imputation)\n",
    "\n",
    "Initially, the dataset only contained two files: `train.csv` and `test.csv`. However, the `test.csv` file didn't include any target labels, and since the HackerEarth competition had already ended, I couldn't use it to evaluate my model's performance.\n",
    "\n",
    "**Solution:**  \n",
    "To address this, I will manually split the original `train.csv` into two separate sets: a new `train_split.csv` and a `validation_split.csv`, using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, validation_split = train_test_split(train_copy, test_size=0.2, random_state=42, stratify=train_copy['churn_risk_score'])\n",
    "\n",
    "train_copy.to_csv('train_cleaned.csv', index=False)\n",
    "train_split.to_csv('train_split_cleaned.csv', index=False)\n",
    "validation_split.to_csv('validation_split_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Handling missing value approach**\n",
    "\n",
    "I will use `verstack.NaNImputer` as it uses a powerful model-based imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the whole cleaning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataCleaner Transformer\n",
    "\n",
    "This transformer:\n",
    "\n",
    "-\tdrops unwanted cols\n",
    "\n",
    "-\treplace unknown categories (e.g., '?') with `np.nan`\n",
    "\n",
    "-\thandle wrong negative values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner(BaseEstimator, TransformerMixin):\n",
    "\tdef __init__(self, cols_to_drop, nonnegative_cols):\n",
    "\t\tself.cols_to_drop = cols_to_drop\n",
    "\t\tself.nonnegative_cols = nonnegative_cols\n",
    "   \n",
    "\tdef fit(self, X, y=None):\n",
    "\t\treturn self\n",
    "\n",
    "\t# X is pd.DataFrame\n",
    "\tdef transform(self, X):\n",
    "\t\tX_copy = X.copy()\n",
    "\t\tX_copy.drop(columns=self.cols_to_drop, errors='ignore', inplace=True)\t\n",
    "\t\t\t\n",
    "\t\tX_copy.replace(['?', 'Error'], np.nan, inplace=True)\n",
    "\t\t\n",
    "\t\tif 'avg_frequency_login_days' in X_copy.columns:\n",
    "\t\t\tX_copy['avg_frequency_login_days'] = X_copy['avg_frequency_login_days'].astype(float)\n",
    "\n",
    "\t\tfor col in self.nonnegative_cols:\n",
    "\t\t\tif col in X_copy.columns:\n",
    "\t\t\t\tX_copy.loc[X_copy[col] < 0, col] = np.nan\n",
    "\n",
    "\t\treturn X_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Wrapping `verstack.NaNImputer` into an custom transformer for compatibility with scikit-learn's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by the Adapter design pattern ;)\n",
    "class NaNImputerWrapper(BaseEstimator, TransformerMixin):\n",
    "\tdef __init__(self, train_sample_size=30_000, verbose=True):\n",
    "\t\tself.train_sample_size = train_sample_size\n",
    "\t\tself.verbose = verbose\n",
    "\t\tself.imputer = NaNImputer(self.train_sample_size, self.verbose)\n",
    "\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, X):\n",
    "\t\treturn self.imputer.impute(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating transformers into a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_train_y_train(df, y='churn_risk_score'):\n",
    "\tX_train = df.drop(columns=[y])\n",
    "\ty_train = df[y]\n",
    "\treturn X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing train_copy for advanced analysis and building dashboards\n",
    "X_train, y_train = get_X_train_y_train(train_copy)\n",
    "\n",
    "# imputing train_split for model development (we avoid imputing the validation set here to avoid data leakage)\n",
    "X_train_split, y_train_split = get_X_train_y_train(train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_pipeline = Pipeline([\n",
    "\t('dataCleaner', DataCleaner(cols_to_drop, nonnegative_cols)), \n",
    "\t('imputer', NaNImputerWrapper(train_sample_size=train_split.shape[0]))\n",
    "])\n",
    "\n",
    "X_train_cleaned_imputed = cleaning_pipeline.fit_transform(X_train)\n",
    "X_train_split_cleaned_imputed = cleaning_pipeline.fit_transform(X_train_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(X, y, name, extension='.csv'):\n",
    "\tdf_cleaned_imputed = pd.concat([X, y], axis=1)\n",
    "\tdf_cleaned_imputed.to_csv(name + extension, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(X_train_cleaned_imputed, y_train, 'train_cleaned_imputed')\n",
    "\n",
    "save_df(X_train_split_cleaned_imputed, y_train_split, 'train_split_cleaned_imputed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save `cleaning_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(cleaning_pipeline, 'cleaning_pipeline.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
